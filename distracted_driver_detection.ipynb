{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Machine Learning Capstone Project\n",
    "\n",
    "## State Farm Distracted Driver Detection (Can computer vision spot distracted drivers?)\n",
    "\n",
    "---\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Data Analysis and Preprocessing\n",
    "* [Step 2](#step2): Create a CNN to classify driver images (from scratch)\n",
    "* [Step 3](#step3): Use a CNN to classify driver images (using transfer learning on pre-trained VGG16)\n",
    "* [Step 4](#step4): Create a CNN to classify driver images (using transfer learning on pre-trained ResNet50)\n",
    "* [Step 5](#step5): Algorithm test result\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Driver Image Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of driver images. We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `input_train_files`, `input_test_files` - numpy arrays containing file paths to images\n",
    "- `input_train_targets`, `input_test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `label_names` - list of string-valued label codes of driver behaviors for translating labels :\n",
    "\n",
    "    * c0: normal driving\n",
    "    * c1: texting - right\n",
    "    * c2: talking on the phone - right\n",
    "    * c3: texting - left\n",
    "    * c4: talking on the phone - left\n",
    "    * c5: operating the radio\n",
    "    * c6: drinking\n",
    "    * c7: reaching behind\n",
    "    * c8: hair and makeup\n",
    "    * c9: talking to passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 102150 total driver images provided by State Farm:\n",
      "22424 train driver images\n",
      "79726 test driver images\n",
      "\n",
      "There are 10 total driver behavior categories.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# define function to load train and test image datasets provided by State Farm\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    driver_files = np.array(data['filenames'])\n",
    "    driver_targets = np_utils.to_categorical(np.array(data['target']), 10)\n",
    "    return driver_files, driver_targets\n",
    "\n",
    "# load original train and test datasets provided by State Farm\n",
    "input_train_files, input_train_targets = load_dataset('imgs/train')\n",
    "input_test_files, input_test_targets = load_dataset('imgs/test')\n",
    "\n",
    "print('There are %s total driver images provided by State Farm:' % len(np.hstack([input_train_files, input_test_files])))\n",
    "print('%d train driver images' % len(input_train_files))\n",
    "print('%d test driver images\\n' % len(input_test_files))\n",
    "\n",
    "# load list of label codes of driver behaviors\n",
    "label_names = [item[11:13] for item in sorted(glob(\"imgs/train/*/\"))]\n",
    "\n",
    "print('There are %d total driver behavior categories.' % len(label_names))\n",
    "\n",
    "# for saving memory\n",
    "del input_test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To avoid running out of memory,\n",
      "we select 8969 images from original train set\n",
      "and ignore the remaining 13455 images.\n",
      "\n",
      "Among the 8969 randomly selected images,\n",
      "we use 7175 images for training and\n",
      "we use 1794 images for validation.\n"
     ]
    }
   ],
   "source": [
    "# Since the number of driver images in original input train dataset provided by State Farm is too large,\n",
    "# for avoidance of running out of memory when we transform them into tensors to feed and train the CNN models,\n",
    "# we randomly sample a particular ratio (0.4) of them to use, and ignore the remaining portion (ratio 0.6).\n",
    "ignore_ratio = 0.6\n",
    "use_files, non_use_files, use_targets, non_use_targets = \\\n",
    "            train_test_split(input_train_files, input_train_targets, test_size=ignore_ratio, random_state=5)\n",
    "\n",
    "print('To avoid running out of memory,')\n",
    "print('we select %s images from original train set' % len(use_files))\n",
    "print('and ignore the remaining %s images.\\n' % len(non_use_files))\n",
    "\n",
    "# shuffle and split the sampled \"use\" dataset into training set (80%) and validation set (20%)\n",
    "train_files, valid_files, train_targets, valid_targets = \\\n",
    "            train_test_split(use_files, use_targets, test_size=0.2, random_state=5)\n",
    "\n",
    "print('Among the %s randomly selected images,' % len(use_files))\n",
    "print('we use %d images for training and' % len(train_files))\n",
    "print('we use %d images for validation.' % len(valid_files))\n",
    "\n",
    "# for saving memory\n",
    "del use_files, non_use_files, use_targets, non_use_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a list of 79726 test image filenames\n",
    "test_image_filename_list = [test_file_path[15:] for test_file_path in input_test_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Data Analysis and Preprocessing\n",
    "\n",
    "### Data Analysis\n",
    "\n",
    "In the code cells below, we read the **driver_imgs_list.csv** file provided by State Farm. This csv file is a list of original input training images, their `subject` (driver id), and `classname` (label id). We then analyze this original input train data set. There is just a little bit size imbalance between different classes, i.e., roughly balance. Note that class 'c0' has the maximum number of 2489 images and class 'c8' has the minimum number of 1911 images among all the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_44733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_72999.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_25094.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_69092.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_92629.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject classname            img\n",
       "0    p002        c0  img_44733.jpg\n",
       "1    p002        c0  img_72999.jpg\n",
       "2    p002        c0  img_25094.jpg\n",
       "3    p002        c0  img_69092.jpg\n",
       "4    p002        c0  img_92629.jpg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "training_images_df = pd.read_csv(\"driver_imgs_list.csv\")\n",
    "# display the top 5 data items in \"driver_imgs_list.csv\"\n",
    "display(training_images_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22424</td>\n",
       "      <td>22424</td>\n",
       "      <td>22424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>22424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>p021</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_4530.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1237</td>\n",
       "      <td>2489</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject classname           img\n",
       "count    22424     22424         22424\n",
       "unique      26        10         22424\n",
       "top       p021        c0  img_4530.jpg\n",
       "freq      1237      2489             1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display information of original input train images\n",
    "display(training_images_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c0    2489\n",
      "c3    2346\n",
      "c4    2326\n",
      "c6    2325\n",
      "c2    2317\n",
      "c5    2312\n",
      "c1    2267\n",
      "c9    2129\n",
      "c7    2002\n",
      "c8    1911\n",
      "Name: classname, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# show the 10 classes of driver images and the number of images in each class in the original input train data set\n",
    "print(training_images_df['classname'].value_counts(sort=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, we rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7175/7175 [00:56<00:00, 127.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1794/1794 [00:13<00:00, 129.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "\n",
    "# the 79726 test images will lead to run out of memory\n",
    "#test_tensors = paths_to_tensor(input_test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Create a CNN to classify driver images (from scratch)\n",
    "\n",
    "We will use Keras and Tensorflow to implement our first CNN model. In this step, we will\n",
    "provide the architecture of the first CNN model we design. And then test the performance\n",
    "result of this CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "We create a CNN to classify driver behaviors. At the end of code cell block, we summarize the layers of the CNN model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We use three convolotion layers followed by three max pooling layers interleavingly and then use two fully connected layers behind in the CNN architecture. We also adopt batch_normalization layers between each convolution layer or dense layer and their activation layer to avoid covariate shift and to accelerate the training process. The number of filters in each convolution layer is twice to the previous one (this is a common practice), and we choose 16, 32, and 64 filters to extract the feature maps (regional information). The window size of feature filter in each convolution layer and also the pool size in each max pooling layer are both (2,2), and it's also a kind of typical choices. We set the padding parameter to be 'same' for not lost information near matrix boundaries. The activation function in each layer beside output is ReLU for dealing with the vanishing gradient problem, and that in output layer is SoftMax for calculation of probabilities on the multi-classes. In max pooling layers, we set the strides parameter to be 2 for half both length and width of each 2D feature map (dimensional reduction), and such strides setting is also typical. Before fully connected layers, we use the GlobalAveragePooling2D layer, which can immediately reduce the amount of parameters and avoid overfitting as well as save much time. We adopt the dropout layers with probability 0.2 to reduce opportunity of overfitting. We choose the number of nodes to be 64 in the first fully connected layer for an initial try, and due to the 10 classes of driver behaviors, the number of nodes in output layer is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 112, 112, 16)      64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 56, 56, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 16,098\n",
      "Trainable params: 15,726\n",
      "Non-trainable params: 372\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', input_shape=(224, 224, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "We choose RMSprop optimizer (RMS: root mean squared error). It decreases the learning rate by dividing it by an exponentially decaying average of squared gradients. We use categorical cross entropy to be the loss error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "We train the first CNN model we design in the code cell below. We adopt `epochs` parameter to be 10 and `batch_size` parameter to be 20 for an initial try. Use model checkpointing to save the model that attains the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7175 samples, validate on 1794 samples\n",
      "Epoch 1/10\n",
      "7160/7175 [============================>.] - ETA: 1s - loss: 2.2045 - acc: 0.1996Epoch 00000: val_loss improved from inf to 3.07245, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "7175/7175 [==============================] - 1042s - loss: 2.2047 - acc: 0.1994 - val_loss: 3.0725 - val_acc: 0.1165\n",
      "Epoch 2/10\n",
      "7160/7175 [============================>.] - ETA: 1s - loss: 1.8863 - acc: 0.3193Epoch 00001: val_loss did not improve\n",
      "7175/7175 [==============================] - 1037s - loss: 1.8863 - acc: 0.3192 - val_loss: 3.1912 - val_acc: 0.1600\n",
      "Epoch 3/10\n",
      "7160/7175 [============================>.] - ETA: 1s - loss: 1.7288 - acc: 0.3851Epoch 00002: val_loss improved from 3.07245 to 2.58792, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "7175/7175 [==============================] - 1041s - loss: 1.7287 - acc: 0.3851 - val_loss: 2.5879 - val_acc: 0.2469\n",
      "Epoch 4/10\n",
      "7160/7175 [============================>.] - ETA: 1s - loss: 1.6152 - acc: 0.4284Epoch 00003: val_loss improved from 2.58792 to 2.14433, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "7175/7175 [==============================] - 1047s - loss: 1.6151 - acc: 0.4283 - val_loss: 2.1443 - val_acc: 0.2759\n",
      "Epoch 5/10\n",
      "7160/7175 [============================>.] - ETA: 2s - loss: 1.5257 - acc: 0.4718Epoch 00004: val_loss did not improve\n",
      "7175/7175 [==============================] - 1076s - loss: 1.5264 - acc: 0.4714 - val_loss: 2.8780 - val_acc: 0.2336\n",
      "Epoch 6/10\n",
      "7160/7175 [============================>.] - ETA: 1s - loss: 1.4479 - acc: 0.4990Epoch 00005: val_loss did not improve\n",
      "7175/7175 [==============================] - 1001s - loss: 1.4474 - acc: 0.4994 - val_loss: 2.8416 - val_acc: 0.2464\n",
      "Epoch 7/10\n",
      "7160/7175 [============================>.] - ETA: 1s - loss: 1.3517 - acc: 0.5342Epoch 00006: val_loss improved from 2.14433 to 1.75698, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "7175/7175 [==============================] - 1015s - loss: 1.3518 - acc: 0.5339 - val_loss: 1.7570 - val_acc: 0.3685\n",
      "Epoch 8/10\n",
      "7160/7175 [============================>.] - ETA: 1s - loss: 1.2981 - acc: 0.5594Epoch 00007: val_loss did not improve\n",
      "7175/7175 [==============================] - 1028s - loss: 1.2977 - acc: 0.5596 - val_loss: 3.4008 - val_acc: 0.2508\n",
      "Epoch 9/10\n",
      "7160/7175 [============================>.] - ETA: 1s - loss: 1.2109 - acc: 0.5888Epoch 00008: val_loss did not improve\n",
      "7175/7175 [==============================] - 966s - loss: 1.2110 - acc: 0.5887 - val_loss: 4.3637 - val_acc: 0.2057\n",
      "Epoch 10/10\n",
      "7160/7175 [============================>.] - ETA: 1s - loss: 1.1889 - acc: 0.5918Epoch 00009: val_loss did not improve\n",
      "7175/7175 [==============================] - 954s - loss: 1.1894 - acc: 0.5918 - val_loss: 3.0435 - val_acc: 0.2486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x257e40c45f8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=10, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "In the code cell below, we test our first CNN model on the testing data set of driver images. The prediction probability results of all the 79726 test images are written into the csv file: **CNN_1_test_probability.csv**, following the submission format defined by Kaggle:\n",
    "\n",
    "- img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9\n",
    "- img_0.jpg,1,0,0,0,0,...,0\n",
    "- img_1.jpg,0.3,0.1,0.6,0,...,0\n",
    "- ...\n",
    "\n",
    "**The score (evaluation metrics: logarithmic loss function) of our first CNN model is 2.60072.**\n",
    "\n",
    "**The test result of our first CNN model is ranked 1355 out of 1440 in public leader board, and ranked 1340 out of 1440 in private leader board.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for saving memory\n",
    "del train_tensors, valid_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 79726/79726 [1:16:49<00:00, 17.23it/s]\n"
     ]
    }
   ],
   "source": [
    "driver_behavior_predictions = []\n",
    "for test_file in tqdm(input_test_files):\n",
    "    test_tensor = path_to_tensor(test_file)\n",
    "    test_tensor = np.vstack(test_tensor).astype('float32')/255\n",
    "    driver_behavior_predictions.append(model.predict(np.expand_dims(test_tensor, axis=0))[0])\n",
    "\n",
    "#driver_behavior_predictions = [model.predict(np.expand_dims(test_tensor, axis=0))[0] for test_tensor in test_tensors]\n",
    "\n",
    "test_image_probability_csv = np.column_stack((np.asarray(test_image_filename_list), \\\n",
    "                                              np.asarray(driver_behavior_predictions, dtype=np.float32)))\n",
    "\n",
    "np.savetxt('submission/CNN_1_test_probability.csv', test_image_probability_csv, delimiter=',', \\\n",
    "           comments='', newline='\\n', fmt='%s', header='img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9')\n",
    "\n",
    "# get index of predicted dog breed for each image in test set\n",
    "#dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "#test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "#print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for saving memory\n",
    "del driver_behavior_predictions, test_image_probability_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Use a CNN to classify driver images (using transfer learning on pre-trained VGG16)\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we will train a CNN model using\n",
    "transfer learning. In this step, our CNN model will use the pre-trained VGG16 model as a\n",
    "**fixed feature extractor**, where the last convolutional output of VGG16 is fed as input to our\n",
    "model.\n",
    "\n",
    "## VGG16\n",
    "### Import VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# https://keras.io/applications/#vgg16\n",
    "# NOT include the 3 fully-connected layers at the top of the network\n",
    "model = VGG16(include_top=False)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1794/1794 [50:22<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "bottleneck_features_valid = []\n",
    "\n",
    "for img_path in tqdm(valid_files):\n",
    "    tensor = path_to_tensor(img_path)\n",
    "    tensor = np.vstack(tensor).astype('float32')/255\n",
    "    bottleneck_features_valid.append(model.predict(np.expand_dims(tensor, axis=0))[0])\n",
    "\n",
    "bottleneck_features_valid = np.asarray(bottleneck_features_valid, dtype=np.float32)\n",
    "\n",
    "#bottleneck_features_valid = \\\n",
    "#        np.asarray([model.predict(np.expand_dims(valid_tensor, axis=0))[0] for valid_tensor in valid_tensors], dtype=np.float32)\n",
    "\n",
    "np.save(open('bottleneck_features/driver_VGG16_valid.npy', 'wb'), bottleneck_features_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for saving memory\n",
    "del bottleneck_features_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7175/7175 [3:35:43<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "bottleneck_features_train = []\n",
    "\n",
    "for img_path in tqdm(train_files):\n",
    "    tensor = path_to_tensor(img_path)\n",
    "    tensor = np.vstack(tensor).astype('float32')/255\n",
    "    bottleneck_features_train.append(model.predict(np.expand_dims(tensor, axis=0))[0])\n",
    "\n",
    "bottleneck_features_train = np.asarray(bottleneck_features_train, dtype=np.float32)\n",
    "\n",
    "#bottleneck_features_train = \\\n",
    "#        np.asarray([model.predict(np.expand_dims(train_tensor, axis=0))[0] for train_tensor in train_tensors], dtype=np.float32)\n",
    "\n",
    "np.save(open('bottleneck_features/driver_VGG16_train.npy', 'wb'), bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for saving memory\n",
    "del bottleneck_features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bottleneck_features_valid = np.load('bottleneck_features/driver_VGG16_valid.npy')\n",
    "bottleneck_features_train = np.load('bottleneck_features/driver_VGG16_train.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "In our second CNN model using transfer learning of VGG16, we add a global average pooling (GAP) layer and two fully connected (dense) layers, where the latter dense layer contains 10 nodes for each of the 10 driver behavior categories, and the output layer is equipped with a softmax activation function to calculate the probabilities for multi-classes (10 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 33,778\n",
      "Trainable params: 33,630\n",
      "Non-trainable params: 148\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "VGG16_model = Sequential()\n",
    "\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=bottleneck_features_train.shape[1:]))\n",
    "\n",
    "VGG16_model.add(Dense(64))\n",
    "VGG16_model.add(BatchNormalization())\n",
    "VGG16_model.add(Activation('relu'))\n",
    "VGG16_model.add(Dropout(0.2))\n",
    "\n",
    "VGG16_model.add(Dense(10))\n",
    "VGG16_model.add(BatchNormalization())\n",
    "VGG16_model.add(Activation('softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "We choose RMSprop optimizer (RMS: root mean squared error). It decreases the learning rate by dividing it by an exponentially decaying average of squared gradients. We use categorical cross entropy to be the loss error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "We adopt `epochs` parameter to be 30 to reduce the opportunity of overfitting and `batch_size` parameter to be 20 to accelerate the gradient descent iterations. We also use model checkpointing to save the model that attains the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7175 samples, validate on 1794 samples\n",
      "Epoch 1/30\n",
      "7060/7175 [============================>.] - ETA: 0s - loss: 1.4111 - acc: 0.5854Epoch 00000: val_loss improved from inf to 1.59571, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "7175/7175 [==============================] - 4s - loss: 1.4040 - acc: 0.5884 - val_loss: 1.5957 - val_acc: 0.4554\n",
      "Epoch 2/30\n",
      "7080/7175 [============================>.] - ETA: 0s - loss: 0.8151 - acc: 0.8222Epoch 00001: val_loss improved from 1.59571 to 1.33407, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "7175/7175 [==============================] - 3s - loss: 0.8134 - acc: 0.8226 - val_loss: 1.3341 - val_acc: 0.5312\n",
      "Epoch 3/30\n",
      "7100/7175 [============================>.] - ETA: 0s - loss: 0.6104 - acc: 0.8720Epoch 00002: val_loss improved from 1.33407 to 0.81983, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "7175/7175 [==============================] - 3s - loss: 0.6101 - acc: 0.8718 - val_loss: 0.8198 - val_acc: 0.7441\n",
      "Epoch 4/30\n",
      "7040/7175 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8888Epoch 00003: val_loss improved from 0.81983 to 0.53228, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "7175/7175 [==============================] - 3s - loss: 0.5155 - acc: 0.8888 - val_loss: 0.5323 - val_acc: 0.8562\n",
      "Epoch 5/30\n",
      "7080/7175 [============================>.] - ETA: 0s - loss: 0.4552 - acc: 0.8945Epoch 00004: val_loss improved from 0.53228 to 0.42591, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "7175/7175 [==============================] - 3s - loss: 0.4552 - acc: 0.8948 - val_loss: 0.4259 - val_acc: 0.8963\n",
      "Epoch 6/30\n",
      "7160/7175 [============================>.] - ETA: 0s - loss: 0.4068 - acc: 0.9042Epoch 00005: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.4066 - acc: 0.9044 - val_loss: 0.5458 - val_acc: 0.8333\n",
      "Epoch 7/30\n",
      "7160/7175 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.9091Epoch 00006: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.3703 - acc: 0.9090 - val_loss: 0.7237 - val_acc: 0.7659\n",
      "Epoch 8/30\n",
      "7100/7175 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.9145Epoch 00007: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.3328 - acc: 0.9148 - val_loss: 0.4614 - val_acc: 0.8640\n",
      "Epoch 9/30\n",
      "7040/7175 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.9263Epoch 00008: val_loss improved from 0.42591 to 0.38260, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "7175/7175 [==============================] - 2s - loss: 0.3050 - acc: 0.9257 - val_loss: 0.3826 - val_acc: 0.8779\n",
      "Epoch 10/30\n",
      "7060/7175 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9232Epoch 00009: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.2973 - acc: 0.9238 - val_loss: 0.5090 - val_acc: 0.8556\n",
      "Epoch 11/30\n",
      "7040/7175 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9391Epoch 00010: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.2583 - acc: 0.9383 - val_loss: 1.0051 - val_acc: 0.6633\n",
      "Epoch 12/30\n",
      "7140/7175 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9416Epoch 00011: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.2390 - acc: 0.9417 - val_loss: 0.4084 - val_acc: 0.8673\n",
      "Epoch 13/30\n",
      "7100/7175 [============================>.] - ETA: 0s - loss: 0.2391 - acc: 0.9373Epoch 00012: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.2385 - acc: 0.9377 - val_loss: 0.5911 - val_acc: 0.8060\n",
      "Epoch 14/30\n",
      "7040/7175 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9428Epoch 00013: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.2235 - acc: 0.9433 - val_loss: 0.5524 - val_acc: 0.8122\n",
      "Epoch 15/30\n",
      "7080/7175 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9469Epoch 00014: val_loss improved from 0.38260 to 0.31964, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "7175/7175 [==============================] - 2s - loss: 0.2063 - acc: 0.9472 - val_loss: 0.3196 - val_acc: 0.8963\n",
      "Epoch 16/30\n",
      "7060/7175 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9500Epoch 00015: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.1923 - acc: 0.9497 - val_loss: 0.4085 - val_acc: 0.8685\n",
      "Epoch 17/30\n",
      "7080/7175 [============================>.] - ETA: 0s - loss: 0.1809 - acc: 0.9564Epoch 00016: val_loss improved from 0.31964 to 0.25630, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "7175/7175 [==============================] - 3s - loss: 0.1819 - acc: 0.9560 - val_loss: 0.2563 - val_acc: 0.9270\n",
      "Epoch 18/30\n",
      "7000/7175 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9546Epoch 00017: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.1849 - acc: 0.9541 - val_loss: 0.2877 - val_acc: 0.9225\n",
      "Epoch 19/30\n",
      "7140/7175 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9573Epoch 00018: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.1723 - acc: 0.9572 - val_loss: 0.3019 - val_acc: 0.8991\n",
      "Epoch 20/30\n",
      "7160/7175 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9594Epoch 00019: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.1578 - acc: 0.9592 - val_loss: 0.3728 - val_acc: 0.8913\n",
      "Epoch 21/30\n",
      "7160/7175 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9567Epoch 00020: val_loss improved from 0.25630 to 0.16978, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "7175/7175 [==============================] - 3s - loss: 0.1596 - acc: 0.9565 - val_loss: 0.1698 - val_acc: 0.9560\n",
      "Epoch 22/30\n",
      "7060/7175 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9630Epoch 00021: val_loss did not improve\n",
      "7175/7175 [==============================] - 4s - loss: 0.1470 - acc: 0.9632 - val_loss: 0.2251 - val_acc: 0.9392\n",
      "Epoch 23/30\n",
      "7120/7175 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9605Epoch 00022: val_loss improved from 0.16978 to 0.10692, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "7175/7175 [==============================] - 2s - loss: 0.1538 - acc: 0.9604 - val_loss: 0.1069 - val_acc: 0.9777\n",
      "Epoch 24/30\n",
      "7120/7175 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9667Epoch 00023: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.1347 - acc: 0.9666 - val_loss: 0.1836 - val_acc: 0.9487\n",
      "Epoch 25/30\n",
      "7000/7175 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9641Epoch 00024: val_loss improved from 0.10692 to 0.10374, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "7175/7175 [==============================] - 2s - loss: 0.1327 - acc: 0.9646 - val_loss: 0.1037 - val_acc: 0.9766\n",
      "Epoch 26/30\n",
      "7020/7175 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9681Epoch 00025: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.1317 - acc: 0.9681 - val_loss: 0.2566 - val_acc: 0.9214\n",
      "Epoch 27/30\n",
      "7020/7175 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9670Epoch 00026: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.1283 - acc: 0.9666 - val_loss: 0.3159 - val_acc: 0.8941\n",
      "Epoch 28/30\n",
      "7100/7175 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9686Epoch 00027: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.1222 - acc: 0.9684 - val_loss: 0.2958 - val_acc: 0.9086\n",
      "Epoch 29/30\n",
      "7120/7175 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9704Epoch 00028: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.1172 - acc: 0.9705 - val_loss: 0.1797 - val_acc: 0.9448\n",
      "Epoch 30/30\n",
      "7020/7175 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9679Epoch 00029: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.1198 - acc: 0.9678 - val_loss: 0.1710 - val_acc: 0.9482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1977f66be80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(bottleneck_features_train, train_targets, \n",
    "                validation_data=(bottleneck_features_valid, valid_targets),\n",
    "                epochs=30, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "In the code cell below, we test our second CNN model (using transfer learning of VGG16) on the testing data set of driver images. The prediction probability results of all the 79726 test images are written into the csv file: **CNN_VGG16_test_probability.csv**, following the submission format defined by Kaggle:\n",
    "\n",
    "- img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9\n",
    "- img_0.jpg,1,0,0,0,0,...,0\n",
    "- img_1.jpg,0.3,0.1,0.6,0,...,0\n",
    "- ...\n",
    "\n",
    "**The score (evaluation metrics: logarithmic loss function) of our second CNN model (using transfer learning of VGG16) is 1.72885.**\n",
    "\n",
    "**The test result of our second CNN model (using transfer learning of VGG16) is ranked 1004 out of 1440 in public leader board, and ranked 746 out of 1440 in private leader board.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for saving memory\n",
    "del bottleneck_features_train, bottleneck_features_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 79726/79726 [39:08:51<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "driver_behavior_predictions = []\n",
    "for test_file in tqdm(input_test_files):\n",
    "    test_tensor = path_to_tensor(test_file)\n",
    "    test_tensor = np.vstack(test_tensor).astype('float32')/255\n",
    "    test_bottleneck_feature = model.predict(np.expand_dims(test_tensor, axis=0))[0]\n",
    "    driver_behavior_predictions.append(VGG16_model.predict(np.expand_dims(test_bottleneck_feature, axis=0))[0])\n",
    "\n",
    "test_image_probability_csv = np.column_stack((np.asarray(test_image_filename_list), \\\n",
    "                                              np.asarray(driver_behavior_predictions, dtype=np.float32)))\n",
    "\n",
    "np.savetxt('submission/CNN_VGG16_test_probability.csv', test_image_probability_csv, delimiter=',', \\\n",
    "           comments='', newline='\\n', fmt='%s', header='img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9')\n",
    "\n",
    "# get index of predicted dog breed for each image in test set\n",
    "#VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "#test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "#print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for saving memory\n",
    "del driver_behavior_predictions, test_image_probability_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from extract_bottleneck_features import *\n",
    "\n",
    "#def VGG16_predict_breed(img_path):\n",
    "#    # extract bottleneck features\n",
    "#    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "#    # obtain predicted vector\n",
    "#    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "#    # return dog breed that is predicted by the model\n",
    "#    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Create a CNN to classify driver images (using transfer learning on pre-trained ResNet50)\n",
    "\n",
    "In this step, instead of VGG16, we may choose to try another pre-trained model, like VGG19, Resnet50, InceptionV3, or Xception, for different pre-trained model selection of transfer learning. We can compare\n",
    "the chosen pre-trained CNN model with the above one (VGG16) and check the difference and result between their prediction scores.\n",
    "\n",
    "Here we choose ResNet50 to be the pre-trained CNN model of transfer learning below. Note that the remaining parts like number of epochs, optimizer, loss function, fully-connected layers (, etc) are not changed for comparisons of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50\n",
    "### Import ResNet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, None, None, 3) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, None, None, 64 9472        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)    (None, None, None, 64 256         conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, None, None, 64 0           bn_conv1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, None, None, 64 0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)          (None, None, None, 64 4160        max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizatio (None, None, None, 64 256         res2a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, None, None, 64 0           bn2a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)          (None, None, None, 64 36928       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizatio (None, None, None, 64 256         res2a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, None, None, 64 0           bn2a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)          (None, None, None, 25 16640       activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)           (None, None, None, 25 16640       max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizatio (None, None, None, 25 1024        res2a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalization (None, None, None, 25 1024        res2a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, None, None, 25 0           bn2a_branch2c[0][0]              \n",
      "                                                                   bn2a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, None, None, 25 0           add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)          (None, None, None, 64 16448       activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizatio (None, None, None, 64 256         res2b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, None, None, 64 0           bn2b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)          (None, None, None, 64 36928       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizatio (None, None, None, 64 256         res2b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, None, None, 64 0           bn2b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)          (None, None, None, 25 16640       activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizatio (None, None, None, 25 1024        res2b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, None, None, 25 0           bn2b_branch2c[0][0]              \n",
      "                                                                   activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, None, None, 25 0           add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)          (None, None, None, 64 16448       activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizatio (None, None, None, 64 256         res2c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, None, None, 64 0           bn2c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)          (None, None, None, 64 36928       activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizatio (None, None, None, 64 256         res2c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, None, None, 64 0           bn2c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)          (None, None, None, 25 16640       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizatio (None, None, None, 25 1024        res2c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, None, None, 25 0           bn2c_branch2c[0][0]              \n",
      "                                                                   activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, None, None, 25 0           add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)          (None, None, None, 12 32896       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizatio (None, None, None, 12 512         res3a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, None, None, 12 0           bn3a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)          (None, None, None, 12 147584      activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizatio (None, None, None, 12 512         res3a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, None, None, 12 0           bn3a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)          (None, None, None, 51 66048       activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)           (None, None, None, 51 131584      activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizatio (None, None, None, 51 2048        res3a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalization (None, None, None, 51 2048        res3a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, None, None, 51 0           bn3a_branch2c[0][0]              \n",
      "                                                                   bn3a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, None, None, 51 0           add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)          (None, None, None, 12 65664       activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizatio (None, None, None, 12 512         res3b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, None, None, 12 0           bn3b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)          (None, None, None, 12 147584      activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizatio (None, None, None, 12 512         res3b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, None, None, 12 0           bn3b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)          (None, None, None, 51 66048       activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizatio (None, None, None, 51 2048        res3b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, None, None, 51 0           bn3b_branch2c[0][0]              \n",
      "                                                                   activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, None, None, 51 0           add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)          (None, None, None, 12 65664       activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizatio (None, None, None, 12 512         res3c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, None, None, 12 0           bn3c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)          (None, None, None, 12 147584      activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizatio (None, None, None, 12 512         res3c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, None, None, 12 0           bn3c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)          (None, None, None, 51 66048       activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizatio (None, None, None, 51 2048        res3c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, None, None, 51 0           bn3c_branch2c[0][0]              \n",
      "                                                                   activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, None, None, 51 0           add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)          (None, None, None, 12 65664       activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizatio (None, None, None, 12 512         res3d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, None, None, 12 0           bn3d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)          (None, None, None, 12 147584      activation_20[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizatio (None, None, None, 12 512         res3d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, None, None, 12 0           bn3d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)          (None, None, None, 51 66048       activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizatio (None, None, None, 51 2048        res3d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_7 (Add)                      (None, None, None, 51 0           bn3d_branch2c[0][0]              \n",
      "                                                                   activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, None, None, 51 0           add_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)          (None, None, None, 25 131328      activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizatio (None, None, None, 25 1024        res4a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, None, None, 25 0           bn4a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)          (None, None, None, 25 590080      activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizatio (None, None, None, 25 1024        res4a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, None, None, 25 0           bn4a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)          (None, None, None, 10 263168      activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)           (None, None, None, 10 525312      activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizatio (None, None, None, 10 4096        res4a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalization (None, None, None, 10 4096        res4a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_8 (Add)                      (None, None, None, 10 0           bn4a_branch2c[0][0]              \n",
      "                                                                   bn4a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, None, None, 10 0           add_8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)          (None, None, None, 25 262400      activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizatio (None, None, None, 25 1024        res4b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, None, None, 25 0           bn4b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)          (None, None, None, 25 590080      activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizatio (None, None, None, 25 1024        res4b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, None, None, 25 0           bn4b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)          (None, None, None, 10 263168      activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizatio (None, None, None, 10 4096        res4b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_9 (Add)                      (None, None, None, 10 0           bn4b_branch2c[0][0]              \n",
      "                                                                   activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_28 (Activation)       (None, None, None, 10 0           add_9[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)          (None, None, None, 25 262400      activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizatio (None, None, None, 25 1024        res4c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_29 (Activation)       (None, None, None, 25 0           bn4c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)          (None, None, None, 25 590080      activation_29[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizatio (None, None, None, 25 1024        res4c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, None, None, 25 0           bn4c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)          (None, None, None, 10 263168      activation_30[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizatio (None, None, None, 10 4096        res4c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_10 (Add)                     (None, None, None, 10 0           bn4c_branch2c[0][0]              \n",
      "                                                                   activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, None, None, 10 0           add_10[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)          (None, None, None, 25 262400      activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizatio (None, None, None, 25 1024        res4d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, None, None, 25 0           bn4d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)          (None, None, None, 25 590080      activation_32[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizatio (None, None, None, 25 1024        res4d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_33 (Activation)       (None, None, None, 25 0           bn4d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)          (None, None, None, 10 263168      activation_33[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizatio (None, None, None, 10 4096        res4d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_11 (Add)                     (None, None, None, 10 0           bn4d_branch2c[0][0]              \n",
      "                                                                   activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, None, None, 10 0           add_11[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)          (None, None, None, 25 262400      activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizatio (None, None, None, 25 1024        res4e_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, None, None, 25 0           bn4e_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)          (None, None, None, 25 590080      activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizatio (None, None, None, 25 1024        res4e_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, None, None, 25 0           bn4e_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)          (None, None, None, 10 263168      activation_36[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizatio (None, None, None, 10 4096        res4e_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_12 (Add)                     (None, None, None, 10 0           bn4e_branch2c[0][0]              \n",
      "                                                                   activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_37 (Activation)       (None, None, None, 10 0           add_12[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)          (None, None, None, 25 262400      activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizatio (None, None, None, 25 1024        res4f_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_38 (Activation)       (None, None, None, 25 0           bn4f_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)          (None, None, None, 25 590080      activation_38[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizatio (None, None, None, 25 1024        res4f_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_39 (Activation)       (None, None, None, 25 0           bn4f_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)          (None, None, None, 10 263168      activation_39[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizatio (None, None, None, 10 4096        res4f_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_13 (Add)                     (None, None, None, 10 0           bn4f_branch2c[0][0]              \n",
      "                                                                   activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_40 (Activation)       (None, None, None, 10 0           add_13[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)          (None, None, None, 51 524800      activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizatio (None, None, None, 51 2048        res5a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_41 (Activation)       (None, None, None, 51 0           bn5a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)          (None, None, None, 51 2359808     activation_41[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizatio (None, None, None, 51 2048        res5a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_42 (Activation)       (None, None, None, 51 0           bn5a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)          (None, None, None, 20 1050624     activation_42[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)           (None, None, None, 20 2099200     activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizatio (None, None, None, 20 8192        res5a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalization (None, None, None, 20 8192        res5a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_14 (Add)                     (None, None, None, 20 0           bn5a_branch2c[0][0]              \n",
      "                                                                   bn5a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_43 (Activation)       (None, None, None, 20 0           add_14[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)          (None, None, None, 51 1049088     activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizatio (None, None, None, 51 2048        res5b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_44 (Activation)       (None, None, None, 51 0           bn5b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)          (None, None, None, 51 2359808     activation_44[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizatio (None, None, None, 51 2048        res5b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_45 (Activation)       (None, None, None, 51 0           bn5b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)          (None, None, None, 20 1050624     activation_45[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizatio (None, None, None, 20 8192        res5b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_15 (Add)                     (None, None, None, 20 0           bn5b_branch2c[0][0]              \n",
      "                                                                   activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_46 (Activation)       (None, None, None, 20 0           add_15[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)          (None, None, None, 51 1049088     activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizatio (None, None, None, 51 2048        res5c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_47 (Activation)       (None, None, None, 51 0           bn5c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)          (None, None, None, 51 2359808     activation_47[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizatio (None, None, None, 51 2048        res5c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_48 (Activation)       (None, None, None, 51 0           bn5c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)          (None, None, None, 20 1050624     activation_48[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizatio (None, None, None, 20 8192        res5c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_16 (Add)                     (None, None, None, 20 0           bn5c_branch2c[0][0]              \n",
      "                                                                   activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_49 (Activation)       (None, None, None, 20 0           add_16[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)      (None, None, None, 20 0           activation_49[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# https://keras.io/applications/#resnet50\n",
    "# NOT include the fully-connected layer at the top of the network.\n",
    "model = ResNet50(include_top=False)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1794/1794 [35:07<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "bottleneck_features_valid = []\n",
    "\n",
    "for img_path in tqdm(valid_files):\n",
    "    tensor = path_to_tensor(img_path)\n",
    "    tensor = np.vstack(tensor).astype('float32')/255\n",
    "    bottleneck_features_valid.append(model.predict(np.expand_dims(tensor, axis=0))[0])\n",
    "\n",
    "bottleneck_features_valid = np.asarray(bottleneck_features_valid, dtype=np.float32)\n",
    "\n",
    "#bottleneck_features_valid = \\\n",
    "#        np.asarray([model.predict(np.expand_dims(valid_tensor, axis=0))[0] for valid_tensor in valid_tensors], dtype=np.float32)\n",
    "\n",
    "np.save(open('bottleneck_features/driver_ResNet50_valid.npy', 'wb'), bottleneck_features_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for saving memory\n",
    "del bottleneck_features_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7175/7175 [2:17:35<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "bottleneck_features_train = []\n",
    "\n",
    "for img_path in tqdm(train_files):\n",
    "    tensor = path_to_tensor(img_path)\n",
    "    tensor = np.vstack(tensor).astype('float32')/255\n",
    "    bottleneck_features_train.append(model.predict(np.expand_dims(tensor, axis=0))[0])\n",
    "\n",
    "bottleneck_features_train = np.asarray(bottleneck_features_train, dtype=np.float32)\n",
    "\n",
    "#bottleneck_features_train = \\\n",
    "#        np.asarray([model.predict(np.expand_dims(train_tensor, axis=0))[0] for train_tensor in train_tensors], dtype=np.float32)\n",
    "\n",
    "np.save(open('bottleneck_features/driver_ResNet50_train.npy', 'wb'), bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for saving memory\n",
    "del bottleneck_features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bottleneck_features_valid = np.load('bottleneck_features/driver_ResNet50_valid.npy')\n",
    "bottleneck_features_train = np.load('bottleneck_features/driver_ResNet50_train.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 132,082\n",
      "Trainable params: 131,934\n",
      "Non-trainable params: 148\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "ResNet50_model = Sequential()\n",
    "\n",
    "ResNet50_model.add(GlobalAveragePooling2D(input_shape=bottleneck_features_train.shape[1:]))\n",
    "\n",
    "ResNet50_model.add(Dense(64))\n",
    "ResNet50_model.add(BatchNormalization())\n",
    "ResNet50_model.add(Activation('relu'))\n",
    "ResNet50_model.add(Dropout(0.2))\n",
    "\n",
    "ResNet50_model.add(Dense(10))\n",
    "ResNet50_model.add(BatchNormalization())\n",
    "ResNet50_model.add(Activation('softmax'))\n",
    "\n",
    "ResNet50_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "We choose RMSprop optimizer (RMS: root mean squared error). It decreases the learning rate by dividing it by an exponentially decaying average of squared gradients. We use categorical cross entropy to be the loss error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResNet50_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "We adopt `epochs` parameter to be 30 to reduce the opportunity of overfitting and `batch_size` parameter to be 20 to accelerate the gradient descent iterations. We also use model checkpointing to save the model that attains the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7175 samples, validate on 1794 samples\n",
      "Epoch 1/30\n",
      "7100/7175 [============================>.] - ETA: 0s - loss: 1.6050 - acc: 0.4879Epoch 00000: val_loss improved from inf to 2.71717, saving model to saved_models/weights.best.ResNet50.hdf5\n",
      "7175/7175 [==============================] - 5s - loss: 1.6014 - acc: 0.4893 - val_loss: 2.7172 - val_acc: 0.2469\n",
      "Epoch 2/30\n",
      "7140/7175 [============================>.] - ETA: 0s - loss: 1.0839 - acc: 0.7102Epoch 00001: val_loss did not improve\n",
      "7175/7175 [==============================] - 4s - loss: 1.0840 - acc: 0.7102 - val_loss: 5.1464 - val_acc: 0.2012\n",
      "Epoch 3/30\n",
      "7140/7175 [============================>.] - ETA: 0s - loss: 0.8838 - acc: 0.7683Epoch 00002: val_loss did not improve\n",
      "7175/7175 [==============================] - 4s - loss: 0.8839 - acc: 0.7682 - val_loss: 4.6469 - val_acc: 0.2419\n",
      "Epoch 4/30\n",
      "7140/7175 [============================>.] - ETA: 0s - loss: 0.7437 - acc: 0.7990Epoch 00003: val_loss improved from 2.71717 to 1.72089, saving model to saved_models/weights.best.ResNet50.hdf5\n",
      "7175/7175 [==============================] - 4s - loss: 0.7427 - acc: 0.7994 - val_loss: 1.7209 - val_acc: 0.4387\n",
      "Epoch 5/30\n",
      "7100/7175 [============================>.] - ETA: 0s - loss: 0.6574 - acc: 0.8235Epoch 00004: val_loss did not improve\n",
      "7175/7175 [==============================] - 4s - loss: 0.6576 - acc: 0.8234 - val_loss: 6.3729 - val_acc: 0.2090\n",
      "Epoch 6/30\n",
      "7160/7175 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.8331Epoch 00005: val_loss did not improve\n",
      "7175/7175 [==============================] - 4s - loss: 0.6027 - acc: 0.8329 - val_loss: 4.6145 - val_acc: 0.3322\n",
      "Epoch 7/30\n",
      "7160/7175 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.8542Epoch 00006: val_loss did not improve\n",
      "7175/7175 [==============================] - 4s - loss: 0.5454 - acc: 0.8539 - val_loss: 2.2455 - val_acc: 0.3846\n",
      "Epoch 8/30\n",
      "7140/7175 [============================>.] - ETA: 0s - loss: 0.4940 - acc: 0.8643Epoch 00007: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.4929 - acc: 0.8648 - val_loss: 1.7672 - val_acc: 0.4476\n",
      "Epoch 9/30\n",
      "7160/7175 [============================>.] - ETA: 0s - loss: 0.4803 - acc: 0.8649Epoch 00008: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.4805 - acc: 0.8648 - val_loss: 4.9205 - val_acc: 0.2202\n",
      "Epoch 10/30\n",
      "7120/7175 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.8685Epoch 00009: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.4492 - acc: 0.8679 - val_loss: 3.4674 - val_acc: 0.3149\n",
      "Epoch 11/30\n",
      "7060/7175 [============================>.] - ETA: 0s - loss: 0.4328 - acc: 0.8688Epoch 00010: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.4321 - acc: 0.8690 - val_loss: 2.1348 - val_acc: 0.5190\n",
      "Epoch 12/30\n",
      "7140/7175 [============================>.] - ETA: 0s - loss: 0.4021 - acc: 0.8831Epoch 00011: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.4017 - acc: 0.8831 - val_loss: 3.5182 - val_acc: 0.3194\n",
      "Epoch 13/30\n",
      "7100/7175 [============================>.] - ETA: 0s - loss: 0.3998 - acc: 0.8831Epoch 00012: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.3991 - acc: 0.8832 - val_loss: 4.4805 - val_acc: 0.2246\n",
      "Epoch 14/30\n",
      "7140/7175 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.8895Epoch 00013: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.3818 - acc: 0.8895 - val_loss: 4.9626 - val_acc: 0.3055\n",
      "Epoch 15/30\n",
      "7080/7175 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8912Epoch 00014: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.3713 - acc: 0.8916 - val_loss: 2.2334 - val_acc: 0.5095\n",
      "Epoch 16/30\n",
      "7080/7175 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8935Epoch 00015: val_loss improved from 1.72089 to 1.22959, saving model to saved_models/weights.best.ResNet50.hdf5\n",
      "7175/7175 [==============================] - 3s - loss: 0.3610 - acc: 0.8932 - val_loss: 1.2296 - val_acc: 0.6577\n",
      "Epoch 17/30\n",
      "7060/7175 [============================>.] - ETA: 0s - loss: 0.3352 - acc: 0.9045Epoch 00016: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.3356 - acc: 0.9045 - val_loss: 3.8870 - val_acc: 0.3785\n",
      "Epoch 18/30\n",
      "7100/7175 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8979Epoch 00017: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.3339 - acc: 0.8973 - val_loss: 5.7812 - val_acc: 0.2224\n",
      "Epoch 19/30\n",
      "7100/7175 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9107Epoch 00018: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.3018 - acc: 0.9112 - val_loss: 3.2609 - val_acc: 0.4643\n",
      "Epoch 20/30\n",
      "7080/7175 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.9081Epoch 00019: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.3062 - acc: 0.9079 - val_loss: 4.4571 - val_acc: 0.3528\n",
      "Epoch 21/30\n",
      "7160/7175 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.9120Epoch 00020: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.3021 - acc: 0.9122 - val_loss: 7.4698 - val_acc: 0.2068\n",
      "Epoch 22/30\n",
      "7000/7175 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9167Epoch 00021: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.2823 - acc: 0.9168 - val_loss: 3.1621 - val_acc: 0.3779\n",
      "Epoch 23/30\n",
      "7140/7175 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.9172Epoch 00022: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.2785 - acc: 0.9174 - val_loss: 6.5179 - val_acc: 0.1828\n",
      "Epoch 24/30\n",
      "7080/7175 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9178Epoch 00023: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.2813 - acc: 0.9172 - val_loss: 5.1432 - val_acc: 0.2698\n",
      "Epoch 25/30\n",
      "7140/7175 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9248Epoch 00024: val_loss did not improve\n",
      "7175/7175 [==============================] - 3s - loss: 0.2602 - acc: 0.9249 - val_loss: 3.0591 - val_acc: 0.4989\n",
      "Epoch 26/30\n",
      "7140/7175 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9265Epoch 00025: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.2582 - acc: 0.9261 - val_loss: 1.8411 - val_acc: 0.5635\n",
      "Epoch 27/30\n",
      "7060/7175 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9235Epoch 00026: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.2535 - acc: 0.9240 - val_loss: 5.3841 - val_acc: 0.2625\n",
      "Epoch 28/30\n",
      "7080/7175 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9213Epoch 00027: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.2602 - acc: 0.9218 - val_loss: 4.4819 - val_acc: 0.3880\n",
      "Epoch 29/30\n",
      "7040/7175 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9315Epoch 00028: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.2346 - acc: 0.9316 - val_loss: 2.9490 - val_acc: 0.4013\n",
      "Epoch 30/30\n",
      "7060/7175 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9346Epoch 00029: val_loss did not improve\n",
      "7175/7175 [==============================] - 2s - loss: 0.2293 - acc: 0.9344 - val_loss: 1.8172 - val_acc: 0.5680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e45a2a59e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.ResNet50.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "ResNet50_model.fit(bottleneck_features_train, train_targets, \n",
    "                   validation_data=(bottleneck_features_valid, valid_targets),\n",
    "                   epochs=30, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ResNet50_model.load_weights('saved_models/weights.best.ResNet50.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "In the code cell below, we test our CNN model using transfer learning of ResNet50 on the testing data set of driver images. The prediction probability results of all the 79726 test images are written into the csv file: **CNN_ResNet50_test_probability.csv**, following the submission format defined by Kaggle:\n",
    "\n",
    "- img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9\n",
    "- img_0.jpg,1,0,0,0,0,...,0\n",
    "- img_1.jpg,0.3,0.1,0.6,0,...,0\n",
    "- ...\n",
    "\n",
    "**The score (evaluation metrics: logarithmic loss function) of our third CNN model (using transfer learning of ResNet50) is 3.08525.**\n",
    "\n",
    "**The test result of our third CNN model (using transfer learning of ResNet50) is ranked 1380 out of 1440 in public leader board, and ranked 1380 out of 1440 in private leader board.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for saving memory\n",
    "del bottleneck_features_train, bottleneck_features_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 79726/79726 [25:44:08<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "driver_behavior_predictions = []\n",
    "for test_file in tqdm(input_test_files):\n",
    "    test_tensor = path_to_tensor(test_file)\n",
    "    test_tensor = np.vstack(test_tensor).astype('float32')/255\n",
    "    test_bottleneck_feature = model.predict(np.expand_dims(test_tensor, axis=0))[0]\n",
    "    driver_behavior_predictions.append(ResNet50_model.predict(np.expand_dims(test_bottleneck_feature, axis=0))[0])\n",
    "\n",
    "test_image_probability_csv = np.column_stack((np.asarray(test_image_filename_list), \\\n",
    "                                              np.asarray(driver_behavior_predictions, dtype=np.float32)))\n",
    "\n",
    "np.savetxt('submission/CNN_ResNet50_test_probability.csv', test_image_probability_csv, delimiter=',', \\\n",
    "           comments='', newline='\\n', fmt='%s', header='img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9')\n",
    "\n",
    "# get index of predicted dog breed for each image in test set\n",
    "#VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "#test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "#print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for saving memory\n",
    "del driver_behavior_predictions, test_image_probability_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def Resnet50_predict_breed(img_path):\n",
    "#    # extract bottleneck features\n",
    "#    # VGG19, Resnet50, InceptionV3, or Xception\n",
    "#    bottleneck_feature = extract_Resnet50(path_to_tensor(img_path))\n",
    "#    # obtain predicted vector\n",
    "#    predicted_vector = Resnet50_model.predict(bottleneck_feature)\n",
    "#    # return dog breed that is predicted by the model\n",
    "#    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Algorithm test result\n",
    "\n",
    "We choose the best test result (the lowest score of the evaluation metric of log-loss error function) among the CNN models we\n",
    "construct above to be the final choice of our proposed algorithm.\n",
    "\n",
    "**The CNN model we construct above with the best (lowest) test log-loss score is: transfer learning on VGG16, and it has the test score of 1.72885.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
